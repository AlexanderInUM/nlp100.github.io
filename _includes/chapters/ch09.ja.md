## 第9章: RNN, CNN, アテンション

### 80. ID番号への変換

問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に`1`，2番目に頻出する単語に`2`，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて`0`とせよ．

### 81. RNNによる予測

ID番号で表現された単語列$$\boldsymbol{x} = (x_1, x_2, \dots, x_T)$$がある．ただし，$$T$$は単語列の長さ，$$x_t \in \mathbb{R}^{V}$$は単語のID番号のone-hot表記である（$$V$$は単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列$$\boldsymbol{x}$$からカテゴリ$$y$$を予測するモデルとして，次式を実装せよ．

$$
h_0 = 0, \\
h_t = {\rm RNN}(W_0 x_t, h_{t-1}), \\
y = {\rm softmax}(W_1 h_T)
$$

ただし，$$W_0 \in \mathbb{R}^{300 \times V}$$は単語埋め込み行列（単語のone-hot表記から単語ベクトルに変換する行列），$$h_t \in \mathbb{R}^{50}$$は時刻$$t$$の隠れ状態ベクトル，$$W_1 \in \mathbb{R}^{4 \times 50}$$は隠れ状態ベクトルからカテゴリを予測するための行列である．
なお，この問題ではモデルの学習を行わず，ランダムに初期化された重み行列で$$y$$を計算するだけでよい．

### 82. 確率的勾配降下法による学習

確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．

### 83. ミニバッチ化・GPU上での学習

問題82のコードを改変し，$$B$$事例ごとに損失・勾配を計算して学習を行えるようにせよ（$$B$$の値は適当に選べ）．また，GPU上で学習を実行せよ．

### 84. 単語ベクトルの導入

事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)）で単語埋め込み行列$$W_0$$を初期化し，学習せよ．

### 85. 双方向RNN・多層化

順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．

$$
h'_{T+1} = 0, \\
h'_t = {\rm RNN'}({\rm emb}(x_t), h'_{t+1}), \\
y = {\rm softmax}(W [h_t; h'_1])
$$

また，RNNを多層化せよ．

### 86. 畳み込みニューラルネットワーク (CNN)

### 87. 確率的勾配降下法によるCNNの学習

### 88. パラメータチューニング

### 89. BERT
